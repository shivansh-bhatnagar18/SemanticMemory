{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-lAx2XQSv3GC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.stats import spearmanr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSAModel:\n",
        "    def __init__(self, num_topics=100):\n",
        "        self.num_topics = num_topics\n",
        "        self.vectorizer = CountVectorizer()\n",
        "        self.svd = TruncatedSVD(n_components=num_topics)\n",
        "        self.doc_topic_matrix = None\n",
        "\n",
        "    def train(self, documents):\n",
        "        # Concatenate lines into a single string for each document\n",
        "        concatenated_documents = [' '.join(doc) for doc in documents]\n",
        "\n",
        "        # Convert the documents into a term-document matrix\n",
        "        term_doc_matrix = self.vectorizer.fit_transform(concatenated_documents)\n",
        "\n",
        "        # Apply LSA using TruncatedSVD\n",
        "        self.doc_topic_matrix = self.svd.fit_transform(term_doc_matrix)\n",
        "\n",
        "    def similarity(self, doc1, doc2):\n",
        "        if self.doc_topic_matrix is None:\n",
        "            raise ValueError(\"Model has not been trained. Call train() first.\")\n",
        "\n",
        "        # Transform documents into topic space\n",
        "        vec1 = self.svd.transform(self.vectorizer.transform([doc1]))\n",
        "        vec2 = self.svd.transform(self.vectorizer.transform([doc2]))\n",
        "\n",
        "        # Calculate cosine similarity between document vectors\n",
        "        similarity = cosine_similarity(vec1, vec2)[0][0]\n",
        "        return similarity"
      ],
      "metadata": {
        "id": "hJE2QiW2v6M7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_similarity(model, documents, word_pairs, human_scores):\n",
        "    predicted_scores = [model.similarity(doc1, doc2) for doc1, doc2 in word_pairs]\n",
        "    spearman_corr, _ = spearmanr(predicted_scores, human_scores)\n",
        "    return spearman_corr"
      ],
      "metadata": {
        "id": "amCdh5w_v-Xg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_cross_validation(model, documents, word_pairs, human_scores, k=5):\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    spearman_correlations = []\n",
        "\n",
        "    for fold, (train_indices, val_indices) in enumerate(kf.split(documents)):\n",
        "        print(f\"\\nFold {fold + 1}\")\n",
        "\n",
        "        # Split the data into training and validation sets\n",
        "        train_documents = [documents[i] for i in train_indices]\n",
        "        val_documents = [documents[i] for i in val_indices]\n",
        "\n",
        "        # Train the LSA model on the training set\n",
        "        model.train(train_documents)\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        spearman_corr = evaluate_similarity(model, val_documents, word_pairs, human_scores)\n",
        "        print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
        "\n",
        "        # Append the correlation for this fold to the list\n",
        "        spearman_correlations.append(spearman_corr)\n",
        "\n",
        "    # Calculate and print the overall average correlation across all folds\n",
        "    overall_avg_corr = np.mean(spearman_correlations)\n",
        "    print(f\"\\nOverall Average Spearman Correlation: {overall_avg_corr:.4f}\")"
      ],
      "metadata": {
        "id": "XyFuW491wC-T"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "\n",
        "lines = []\n",
        "with open('shakespeare.txt', 'r') as f:\n",
        "    for l in f:\n",
        "        lines.append(l)\n",
        "\n",
        "# remove new lines\n",
        "lines = [line.rstrip() for line in lines]\n",
        "# make all characters lower\n",
        "lines = [line.lower() for line in lines]\n",
        "# remove punctuations from each line\n",
        "lines = [line.translate(str.maketrans('', '', string.punctuation)) for line in lines]\n",
        "# tokenize\n",
        "lines = [word_tokenize(line) for line in lines]\n",
        "print(lines[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD3wl6pNwIgI",
        "outputId": "e2e7ad94-b04e-4afe-b428-752aae350643"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['act', 'i'], ['scene', 'i', 'london', 'the', 'palace'], ['enter', 'king', 'henry', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'of', 'westmoreland', 'sir', 'walter', 'blunt', 'and', 'others'], ['so', 'shaken', 'as', 'we', 'are', 'so', 'wan', 'with', 'care'], ['find', 'we', 'a', 'time', 'for', 'frighted', 'peace', 'to', 'pant'], ['and', 'breathe', 'shortwinded', 'accents', 'of', 'new', 'broils'], ['to', 'be', 'commenced', 'in', 'strands', 'afar', 'remote'], ['no', 'more', 'the', 'thirsty', 'entrance', 'of', 'this', 'soil'], ['shall', 'daub', 'her', 'lips', 'with', 'her', 'own', 'childrens', 'blood'], ['nor', 'more', 'shall', 'trenching', 'war', 'channel', 'her', 'fields'], ['nor', 'bruise', 'her', 'flowerets', 'with', 'the', 'armed', 'hoofs'], ['of', 'hostile', 'paces', 'those', 'opposed', 'eyes'], ['which', 'like', 'the', 'meteors', 'of', 'a', 'troubled', 'heaven'], ['all', 'of', 'one', 'nature', 'of', 'one', 'substance', 'bred'], ['did', 'lately', 'meet', 'in', 'the', 'intestine', 'shock'], ['and', 'furious', 'close', 'of', 'civil', 'butchery'], ['shall', 'now', 'in', 'mutual', 'wellbeseeming', 'ranks'], ['march', 'all', 'one', 'way', 'and', 'be', 'no', 'more', 'opposed'], ['against', 'acquaintance', 'kindred', 'and', 'allies'], ['the', 'edge', 'of', 'war', 'like', 'an', 'illsheathed', 'knife']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordsim_path = 'combined.csv'\n",
        "word_sim_data = pd.read_csv(wordsim_path)\n",
        "word_pairs = [(w1,w2) for w1,w2 in zip(word_sim_data['Word 1'], word_sim_data['Word 2'])]\n",
        "human_scores = word_sim_data['Human (mean)'].astype(float)"
      ],
      "metadata": {
        "id": "pmdVPkOjwLzn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsa_model = LSAModel(num_topics=100)\n",
        "k_fold_cross_validation(lsa_model, lines, word_pairs, human_scores, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtY45QtCwFJ0",
        "outputId": "4a93e958-1fe3-4c71-c764-50fc8414cbab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1\n",
            "Spearman Correlation: 0.0142\n",
            "\n",
            "Fold 2\n",
            "Spearman Correlation: -0.0426\n",
            "\n",
            "Fold 3\n",
            "Spearman Correlation: 0.0090\n",
            "\n",
            "Fold 4\n",
            "Spearman Correlation: 0.0380\n",
            "\n",
            "Fold 5\n",
            "Spearman Correlation: 0.0119\n",
            "\n",
            "Fold 6\n",
            "Spearman Correlation: 0.0103\n",
            "\n",
            "Fold 7\n",
            "Spearman Correlation: -0.0127\n",
            "\n",
            "Fold 8\n",
            "Spearman Correlation: 0.0027\n",
            "\n",
            "Fold 9\n",
            "Spearman Correlation: -0.0076\n",
            "\n",
            "Fold 10\n",
            "Spearman Correlation: 0.0069\n",
            "\n",
            "Overall Average Spearman Correlation: 0.0030\n"
          ]
        }
      ]
    }
  ]
}